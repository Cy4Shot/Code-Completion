> Note: All of the source code may be found in the [github repository](https://github.com/Cy4Shot/Code-Completion).

# 1. Dataset

Initially, I had to generate a dataset to test the model on. To do this, I used a simple program I wrote that fetches data from OpenStreetMap, specifically relating to the paths of subway lines on the London Underground. The program then processes and plots this data on a map using matplotlib to generate an geographical map of the London Underground. This was chosen so that I could test:
- The model's ability to replicate common python structures (i.e. function definition, loops, etc.)
- The model's knowledge of niche python libraries (i.e. matplotlib, numpy, osmnx, etc.)
- The model's understanding of the data it is working with (i.e. the OpenStreetMap data)
- The model's understanding of the real world (i.e. the London Underground)

The entire source code for this program can be found in the `src` folder.

I wrote a script to extract a dataset from this source code, located at `datagen.py`. Here, we select a file (in this example, `src/mapper.py`) and extract a certain amount of samples from it. For each sample:
  1. Select a random point in the file.
  2. Save everything before that point as the prefix.
  3. Save everything starting from the next line as the suffix.
  4. The rest of that line should be autocompleted by the model.

This dataset is then saved as `dataset.pkl`. To test the model I generated 85 samples.

> I decided to generate only one line at a time, as this would be simpler for the model to predict, preventing it from getting carried away and hallucinating code. In principle, this isn't necessary, but this small limitation should make it significantly easier to test the small model I have.

# 2. Model
To generate code completions, I used the `starcoder2` model with 3B parameters, as this was the largest model I could get running locally on my machine. The full inference script can be seen in `model.py`. The prompt is built as such, following the starcoder2 format:

```
<fim_prefix>code before<fim_suffix>code after<fim middle>
```

The model then continues this string with the completion. We then sanitize the output by removing everything up until the `<fim middle>` token, and then removing everything after the first newline character. This is then returned as the completion.

A report of all of the completions is then saved in a markdown file which can be found at `report.md`.

# 3. Results
Opening the `report.md` file, we can see the completions generated by the model. The model seems to have a good understanding of the code, and is able to generate completions that are syntactically correct. However, the model does not seem to have a good understanding of the data it is working with, as it does not generate completions that are relevant to the London Underground. One factor that could be affecting this is the size of the model, as 3B parameters is likely not enough to fit in knowledge about the real world data.

It also seems that the model is unable to generate completions about more niche libraries such as `osmnx`. Again, the size of the model might be the limiting factor here, as it is likely that the model has not seen enough examples of code using this library to generate completions for it.

The full annotations of interesting / significant completions are below:

### 3.1 Common Python Structures

While  reviewing the results, I could not find a single example where the model failed to generate a syntactically correct completion. This is a good sign, as it shows that the model is able to replicate common python structures with ease. Some example of successful completions that solely depend on the model's ability to replicate common python structures are:

`ox.plot_figure_ground(G, ax=ax, node_alpha=0, show=Fal` - *Correct*
> Expected 	`se,`
> Predicted	`se,`

`def plot_feature(ta` - *Correct*
> Expected 	`g, value, color):`
> Predicted	`g, value, color):`

`pickle_file.seek(0` - *Correct*
> Expected 	`)`
> Predicted	`)`

`fig, ax = plt.subplots(figsize=figsize` - *Correct*
> Expected 	`)`
> Predicted	`)`

`G = ox.graph_from_bb` - *Correct*
> Expected 	`ox(`
> Predicted	`ox(`

`G = ox.features_from_bbox(bbox=bbox, tags={tag: ` - *Correct*
> Expected 	`value})`
> Predicted	`value})`

`def plot_feature(tag, value, color` - *Correct*
> Expected 	`):`
> Predicted	`):`

### 3.2 Using Python Libraries

Here, the model seems to struggle with generating completions for code that uses niche libraries such as `osmnx`.

In this example, the model does not know about the inner functions of `osmnx`, and instead just replicates another function elsewhere in the code. However, it has provided the correct parameters:

`o` - *Correct syntax, but wrong function called.*
> Expected 	`x.plot_figure_ground(G, ax=ax, node_alpha=0, show=False,`
> Predicted	`x.plot_graph(G, ax=ax, node_alpha=0, show=False,`

The model has successfully predicated that it needs to import a library which is used in the code:

`impor` - *Correct*
> Expected 	`t pickle`
> Predicted	`t pickle`

A successful prediction of using the `osmnx` library, but still likely based on other examples in the code:

`G = ox.graph_from_bbox(b` - *Correct*
> Expected 	`box=bbox, custom_filter='["railway"~"rail"]')`
> Predicted	`box=bbox, custom_filter='["railway"~"rail"]')`


`G = ox.graph_from_bb` - *Correct*
> Expected 	`ox(`
> Predicted	`ox(`

Successful replication of a common python snippet involving a third-party library:

`re` - *Correct*
> Expected 	`turn load_pickle('fig.pickle')`
> Predicted	`turn load_pickle('fig.pickle')`

`return load_pick` - *Correct*
> Expected 	`le('fig.pickle')`
> Predicted	`le('fig.pickle')`

`return load_` - *Correct*
> Expected 	`pickle('fig.pickle')`
> Predicted	`pickle('fig.pickle')`

Incorrect ouptut. This may be due to the truncation of the completion:

`G = ox.graph_from_bbo` - *Incorrect, but likely not model's fault.*
> Expected 	`x(bbox=bbox, custom_filter='["railway"~"rail"]')`
> Predicted	`x(`


### 3.3 Understanding the Data

Occassionally, the model understands what data it has to retrieve from OpenStreetMap, but not how. This is likely due to the model being too small. Here we can see that the model correctly identified that it needs to retrieve natural data from openstreetmap, but it does not know how to do this:

`plot_feature("natural` - *Correct syntax, context missing*
> Expected 	`", ["tree", "wood", "grassland",`
> Predicted	`", ["wood", "scrub", "heath", "grassland",`

`"vineyard", "farm` - *Correct syntax, context missing*
> Expected 	`yard", "recreation_ground", "allotments"], 'parks')`
> Predicted	`"], 'farmland')`

`bbox` - *Correct syntax, wrong usage of OSM*
> Expected 	`=bbox, custom_filter=f'["railway"~"subway"]["name"~"{line}"]')`
> Predicted	`=bbox, custom_filter=f'["railway"~"{line}"]')`

`plot_feature("nat` - *Correct syntax, context missing*
> Expected 	`ural", ["tree", "wood", "grassland",`
> Predicted	`ure", ["tree", "tree_row", "scrub", "peak"], 'parks')`

`plot_feature("landuse", ["meadow", "fo`- *Correct syntax, context missing*
> Expected 	`rest", "orchard", "farmland",`
> Predicted	`rest", "grass", "farmland", "farmyard", "vineyard", "allotments"], 'parks')`

We also see occasional successful examples:

`G = ox.graph_from_bbox(b`- *Correct*
> Expected 	`box=bbox, custom_filter='["railway"~"rail"]')`
> Predicted	`box=bbox, custom_filter='["railway"~"rail"]')`

However, they are cut short by attempts to access the correct data, but without understanding how OpenStreetMap is structured:

`bbox=bbox, custom_filter=f'["rail` - *Correct syntax, wrong usage of OSM*
> Expected 	`way"~"subway"]["name"~"{line}"]')`
> Predicted	`way"~"subway"]')`

`G = ox.graph_from_bbox(bbox=bbox, custom_filter='["railway"~"rail"` - *Correct syntax, wrong usage of OSM*
> Expected 	`]')`
> Predicted	`]["name"~"underground"]')`

`close=False, street_widths=street_widths, color=co` - *Correct syntax, wrong usage of OSM*
> Expected 	`lors["highways"], dpi=300, figsize=figsize)`
> Predicted	`lors["streets"], figsize=figsize)`

Occasionally, the model also find's itself carried away, and finds itself in a loop of generating unnecessary completions. This could be due to the configuration of the model. It may be possible to fix this by decreasing the temperature of the model, or by introducing top-k sampling:

`plot_featur` - *Incorrect*
> Expected 	`e("natural", "water", 'water')`
> Predicted	`e("building", ["yes", "residential", "commercial", "industrial", "retail", "hotel", "apartments", "house", "houseboat", "farm", "garages", "garage", "houses", "house_ruins", "house_boats", "house_garden", "house_garden_ruins", "house_garden_boats", "house_garden_ruins_boats", `

``
> Expected 	`bbox=bbox, custom_filter='["highway"~"motorway|trunk|primary"]')` - *Incorrect*
> Predicted	`bbox=bbox, custom_filter='["highway"~"motorway|trunk|primary|secondary|tertiary|unclassified|residential|living_street|service|pedestrian|footway|path|steps|cycleway|track|bridleway|proposed|construction|abandoned|raceway|escape|raceway|road|road|road|road|road|road|road|road|road|roa`

`bbox` - *Incorrect*
> Expected 	`=bbox, custom_filter='["highway"~"motorway|trunk|primary"]')`
> Predicted	`=bbox, custom_filter='["highway"~"motorway|trunk|primary|secondary|tertiary|unclassified|residential|living_street|service|pedestrian|footway|path|steps|cycleway|track|bridleway|proposed|construction|abandoned|raceway|escape|raceway|road|road|road|road|road|road|road|road|road|road|roa`

# 4. Metrics
To provide a measure for the success of the model, I will use four different metrics:
1. **Exact Match**: Useful for finding how many completions are exactly right.
2. **chrF**: Useful to find how many completions are partially correct. In code, small differences in completion may not necessarily be wrong.
3. **Levenshtein Distance**: Useful to understand how much effort a developer would have to put in to correct the completion.
4. **BLEU Score**: Useful to find the overall similarity between the completion and the expected output.

The source code for calculating the metrics can be seen at `metrics.py`. The results of the metrics are as follows:

| Metric        | Value     |
|---------------|-----------|
| Exact Match   | **0.506** |
| chrF          | **0.675** |
| Levenshtein   | **0.789** |
| BLEU          | **0.323** |


1. **Exact Match** - Although surprisingly high for a 3B parameter model, this is a good metric for the model's accuracy in common and menial tasks. The model seems to be able to replicate common python structures with ease.
2. **chrF** - Unlike exact match, the chrF metric is more lenient, and allows for small differences in completions. This may be a good metric to replicate how a human would judge the results, as small differences in variable naming or function calls may not necessarily be wrong. However, this metric should still be used with caution, as it may not always be accurate. For example, a missing `;` may be a very important error, but chrF would not severly penalize this.
3. **Levenshtein** - This metric describes the effort required to fix an incorrect compleion. The high value suggests that the model is able to generate completions that are close to the expected output, but this metric once again does not account for the importance of some slight errors. 
4. **BLEU** - This metric is useful to understand the overall similarity between the completion and the expected output. This metric is the only one tested that provides a negative sentiment (i.e. $<0.5$). This is likely due to the model not being able to generate completions that are relevant to the data it is working with, or not understand real world data. This may be an important metric to consider depending on the use case of the model.

In conclusion, I believe that the **Levenstein** method is the closest to a developer's judgement of the code completions. However, neither of these metrics account for all the nuances of code completion, and should be used in conjunction with each other to paint a full picture of the model's performance.

# 5. Conclusion
During this project, I successfully used my first code-completion model. I learnt about new ways to evaluate the performance of an AI model, and was able to spot interesting edge cases and things to account for in the process. In the future, I would expand this project by generating multiline completions, using a more advanced model (with significantly more parameters), as well as expanding the context window to include the entire source directory of the project, and maybe even a history. All of these features would allow the model to generate significantly more accurate completions.